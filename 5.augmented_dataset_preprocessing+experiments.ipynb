{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a33bb6ea-3fed-4cb1-be59-1bc4a93b9897",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras import datasets, layers, models, losses, Model\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "#!pip install tensorcross\n",
    "from tensorcross.model_selection import GridSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7541fc8-5f60-4291-8d05-3e67ee82ddc4",
   "metadata": {},
   "source": [
    "### Training the data\n",
    "\n",
    "We will begin witht the very first network we trained, and check if by seeing more data, it can perform a better job at classifying our 4 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85b4e77e-b549-4e5e-acf1-174091dc2ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up some configuration variables\n",
    "TRAIN_IMAGE_DIR = 'augmented_dataset_resized/Training'\n",
    "TEST_IMAGE_DIR = 'augmented_dataset_resized/Testing'\n",
    "img_height=167\n",
    "img_width=167\n",
    "batch_size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ed0d471-1d82-4ece-9997-e81a81fea8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8582 files belonging to 4 classes.\n",
      "Using 7467 files for training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-14 12:03:21.230189: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-12-14 12:03:21.230229: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-12-14 12:03:21.230255: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (c100.local): /proc/driver/nvidia/version does not exist\n",
      "2022-12-14 12:03:21.230708: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  TRAIN_IMAGE_DIR,\n",
    "  validation_split=0.13,\n",
    "  subset=\"training\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ab1d11e-d473-4521-82f8-0f425f03e324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8582 files belonging to 4 classes.\n",
      "Using 1716 files for validation.\n"
     ]
    }
   ],
   "source": [
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  TRAIN_IMAGE_DIR,\n",
    "  validation_split=0.2,\n",
    "  subset=\"validation\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aff881e6-8964-43a2-acb6-1fc7646027b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1705 files belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "test_ds=tf.keras.utils.image_dataset_from_directory(\n",
    "  TEST_IMAGE_DIR,\n",
    "  seed=123,\n",
    "  shuffle=False,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)\n",
    "\n",
    "num_classes = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3d89a0b-ebd2-4552-a202-fc97bef7bacc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "234/234 [==============================] - 43s 183ms/step - loss: 0.6615 - accuracy: 0.7234 - val_loss: 0.4162 - val_accuracy: 0.8473\n",
      "Epoch 2/50\n",
      "234/234 [==============================] - 42s 178ms/step - loss: 0.3030 - accuracy: 0.8813 - val_loss: 0.2339 - val_accuracy: 0.9003\n",
      "Epoch 3/50\n",
      "234/234 [==============================] - 42s 177ms/step - loss: 0.1594 - accuracy: 0.9434 - val_loss: 0.2172 - val_accuracy: 0.9038\n",
      "Epoch 4/50\n",
      "234/234 [==============================] - 43s 182ms/step - loss: 0.0928 - accuracy: 0.9658 - val_loss: 0.1130 - val_accuracy: 0.9645\n",
      "Epoch 5/50\n",
      "234/234 [==============================] - 43s 184ms/step - loss: 0.0656 - accuracy: 0.9768 - val_loss: 0.1271 - val_accuracy: 0.9627\n",
      "Epoch 6/50\n",
      "234/234 [==============================] - 43s 182ms/step - loss: 0.0295 - accuracy: 0.9897 - val_loss: 0.1068 - val_accuracy: 0.9738\n",
      "Epoch 7/50\n",
      "234/234 [==============================] - 43s 185ms/step - loss: 0.0172 - accuracy: 0.9941 - val_loss: 0.1068 - val_accuracy: 0.9755\n",
      "Epoch 8/50\n",
      "234/234 [==============================] - 43s 182ms/step - loss: 0.0299 - accuracy: 0.9910 - val_loss: 0.1343 - val_accuracy: 0.9697\n",
      "Epoch 9/50\n",
      "234/234 [==============================] - 42s 181ms/step - loss: 0.0301 - accuracy: 0.9910 - val_loss: 0.0982 - val_accuracy: 0.9738\n",
      "Epoch 10/50\n",
      "234/234 [==============================] - 43s 183ms/step - loss: 0.0127 - accuracy: 0.9963 - val_loss: 0.1126 - val_accuracy: 0.9767\n",
      "Epoch 11/50\n",
      "234/234 [==============================] - 43s 182ms/step - loss: 0.0232 - accuracy: 0.9922 - val_loss: 0.1069 - val_accuracy: 0.9773\n",
      "Epoch 12/50\n",
      "234/234 [==============================] - 43s 183ms/step - loss: 0.0112 - accuracy: 0.9965 - val_loss: 0.1034 - val_accuracy: 0.9790\n",
      "Epoch 13/50\n",
      "234/234 [==============================] - 42s 181ms/step - loss: 0.0095 - accuracy: 0.9968 - val_loss: 0.1427 - val_accuracy: 0.9738\n",
      "Epoch 14/50\n",
      "234/234 [==============================] - 43s 182ms/step - loss: 0.0109 - accuracy: 0.9957 - val_loss: 0.1402 - val_accuracy: 0.9714\n",
      "Epoch 15/50\n",
      "234/234 [==============================] - 43s 182ms/step - loss: 0.0016 - accuracy: 0.9997 - val_loss: 0.1151 - val_accuracy: 0.9790\n",
      "Epoch 16/50\n",
      "234/234 [==============================] - 42s 181ms/step - loss: 1.5587e-04 - accuracy: 1.0000 - val_loss: 0.1099 - val_accuracy: 0.9819\n",
      "Epoch 17/50\n",
      "234/234 [==============================] - 43s 184ms/step - loss: 5.3347e-05 - accuracy: 1.0000 - val_loss: 0.1143 - val_accuracy: 0.9819\n",
      "Epoch 18/50\n",
      "234/234 [==============================] - 42s 180ms/step - loss: 3.6128e-05 - accuracy: 1.0000 - val_loss: 0.1178 - val_accuracy: 0.9808\n",
      "Epoch 19/50\n",
      "234/234 [==============================] - 42s 181ms/step - loss: 2.7229e-05 - accuracy: 1.0000 - val_loss: 0.1210 - val_accuracy: 0.9814\n",
      "Epoch 20/50\n",
      "234/234 [==============================] - 43s 184ms/step - loss: 2.1327e-05 - accuracy: 1.0000 - val_loss: 0.1240 - val_accuracy: 0.9814\n",
      "Epoch 21/50\n",
      "234/234 [==============================] - 43s 184ms/step - loss: 1.7197e-05 - accuracy: 1.0000 - val_loss: 0.1266 - val_accuracy: 0.9814\n",
      "Epoch 22/50\n",
      "234/234 [==============================] - 43s 183ms/step - loss: 1.4009e-05 - accuracy: 1.0000 - val_loss: 0.1291 - val_accuracy: 0.9819\n",
      "Epoch 23/50\n",
      "234/234 [==============================] - 43s 182ms/step - loss: 1.1670e-05 - accuracy: 1.0000 - val_loss: 0.1314 - val_accuracy: 0.9819\n",
      "Epoch 24/50\n",
      "234/234 [==============================] - 43s 182ms/step - loss: 9.7871e-06 - accuracy: 1.0000 - val_loss: 0.1337 - val_accuracy: 0.9819\n",
      "Epoch 25/50\n",
      "234/234 [==============================] - 43s 183ms/step - loss: 8.2554e-06 - accuracy: 1.0000 - val_loss: 0.1359 - val_accuracy: 0.9819\n",
      "Epoch 26/50\n",
      "234/234 [==============================] - 43s 183ms/step - loss: 7.0356e-06 - accuracy: 1.0000 - val_loss: 0.1380 - val_accuracy: 0.9814\n",
      "Epoch 27/50\n",
      "234/234 [==============================] - 43s 183ms/step - loss: 6.0209e-06 - accuracy: 1.0000 - val_loss: 0.1400 - val_accuracy: 0.9814\n",
      "Epoch 28/50\n",
      "234/234 [==============================] - 43s 182ms/step - loss: 5.1778e-06 - accuracy: 1.0000 - val_loss: 0.1421 - val_accuracy: 0.9814\n",
      "Epoch 29/50\n",
      "234/234 [==============================] - 43s 184ms/step - loss: 4.4538e-06 - accuracy: 1.0000 - val_loss: 0.1440 - val_accuracy: 0.9814\n",
      "Epoch 30/50\n",
      "234/234 [==============================] - 43s 183ms/step - loss: 3.8598e-06 - accuracy: 1.0000 - val_loss: 0.1460 - val_accuracy: 0.9814\n",
      "Epoch 31/50\n",
      "234/234 [==============================] - 43s 182ms/step - loss: 3.3598e-06 - accuracy: 1.0000 - val_loss: 0.1478 - val_accuracy: 0.9814\n",
      "Epoch 32/50\n",
      "234/234 [==============================] - 43s 183ms/step - loss: 2.9278e-06 - accuracy: 1.0000 - val_loss: 0.1499 - val_accuracy: 0.9808\n",
      "Epoch 33/50\n",
      "234/234 [==============================] - 43s 185ms/step - loss: 2.5656e-06 - accuracy: 1.0000 - val_loss: 0.1517 - val_accuracy: 0.9808\n",
      "Epoch 34/50\n",
      "234/234 [==============================] - 43s 183ms/step - loss: 2.2474e-06 - accuracy: 1.0000 - val_loss: 0.1536 - val_accuracy: 0.9808\n",
      "Epoch 35/50\n",
      "234/234 [==============================] - 43s 182ms/step - loss: 1.9633e-06 - accuracy: 1.0000 - val_loss: 0.1553 - val_accuracy: 0.9808\n",
      "Epoch 36/50\n",
      "234/234 [==============================] - 43s 183ms/step - loss: 1.7190e-06 - accuracy: 1.0000 - val_loss: 0.1572 - val_accuracy: 0.9808\n",
      "Epoch 37/50\n",
      "234/234 [==============================] - 43s 182ms/step - loss: 1.5134e-06 - accuracy: 1.0000 - val_loss: 0.1590 - val_accuracy: 0.9808\n",
      "Epoch 38/50\n",
      "234/234 [==============================] - 43s 184ms/step - loss: 1.3275e-06 - accuracy: 1.0000 - val_loss: 0.1607 - val_accuracy: 0.9808\n",
      "Epoch 39/50\n",
      "234/234 [==============================] - 43s 183ms/step - loss: 1.1713e-06 - accuracy: 1.0000 - val_loss: 0.1625 - val_accuracy: 0.9808\n",
      "Epoch 40/50\n",
      "234/234 [==============================] - 43s 182ms/step - loss: 1.0328e-06 - accuracy: 1.0000 - val_loss: 0.1643 - val_accuracy: 0.9808\n",
      "Epoch 41/50\n",
      "234/234 [==============================] - 43s 183ms/step - loss: 9.0970e-07 - accuracy: 1.0000 - val_loss: 0.1660 - val_accuracy: 0.9808\n",
      "Epoch 42/50\n",
      "234/234 [==============================] - 43s 185ms/step - loss: 8.0386e-07 - accuracy: 1.0000 - val_loss: 0.1679 - val_accuracy: 0.9808\n",
      "Epoch 43/50\n",
      "234/234 [==============================] - 43s 184ms/step - loss: 7.1006e-07 - accuracy: 1.0000 - val_loss: 0.1696 - val_accuracy: 0.9808\n",
      "Epoch 44/50\n",
      "234/234 [==============================] - 43s 183ms/step - loss: 6.2722e-07 - accuracy: 1.0000 - val_loss: 0.1714 - val_accuracy: 0.9808\n",
      "Epoch 45/50\n",
      "234/234 [==============================] - 43s 183ms/step - loss: 5.5179e-07 - accuracy: 1.0000 - val_loss: 0.1734 - val_accuracy: 0.9808\n",
      "Epoch 46/50\n",
      "234/234 [==============================] - 43s 182ms/step - loss: 4.8517e-07 - accuracy: 1.0000 - val_loss: 0.1752 - val_accuracy: 0.9808\n",
      "Epoch 47/50\n",
      "234/234 [==============================] - 43s 183ms/step - loss: 4.2664e-07 - accuracy: 1.0000 - val_loss: 0.1769 - val_accuracy: 0.9808\n",
      "Epoch 48/50\n",
      "234/234 [==============================] - 43s 182ms/step - loss: 3.7798e-07 - accuracy: 1.0000 - val_loss: 0.1788 - val_accuracy: 0.9808\n",
      "Epoch 49/50\n",
      "234/234 [==============================] - 43s 185ms/step - loss: 3.3096e-07 - accuracy: 1.0000 - val_loss: 0.1806 - val_accuracy: 0.9808\n",
      "Epoch 50/50\n",
      "234/234 [==============================] - 43s 183ms/step - loss: 2.9378e-07 - accuracy: 1.0000 - val_loss: 0.1825 - val_accuracy: 0.9808\n"
     ]
    }
   ],
   "source": [
    "simple_cnn_model = tf.keras.Sequential([\n",
    "  #rescale pixel values to [0,1] interval\n",
    "  tf.keras.layers.Rescaling(1./255),\n",
    "  tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(),\n",
    "  tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(),\n",
    "  tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(num_classes)\n",
    "])\n",
    "\n",
    "simple_cnn_model.compile(\n",
    "  optimizer='adam',\n",
    "  loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "  metrics=['accuracy'])\n",
    "\n",
    "history_simple_cnn=simple_cnn_model.fit(\n",
    "  train_ds,\n",
    "  validation_data=val_ds,\n",
    "  epochs=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a568875f-3c81-475f-b0e8-ad3d4baa7d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracy(model_history, model_name):\n",
    "    plt.plot(model_history.history['accuracy'])\n",
    "    plt.plot(model_history.history['val_accuracy'])\n",
    "    plt.title(model_name+' accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "def plot_loss(model_history, model_name):\n",
    "    plt.plot(model_history.history['loss'])\n",
    "    plt.plot(model_history.history['val_loss'])\n",
    "    plt.title(model_name+' loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26df3661-41f4-49b2-a5c9-92fcd6267e08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9202346041055719"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_accuracy(y_true, y_pred):\n",
    "    correct_predictions = 0\n",
    "    # iterate over each label and check\n",
    "    for true, predicted in zip(y_true, y_pred):\n",
    "        if true == predicted:\n",
    "            correct_predictions += 1\n",
    "    # compute the accuracy\n",
    "    accuracy = correct_predictions/len(y_true)\n",
    "    return accuracy\n",
    "\n",
    "predicted_probabilities=simple_cnn_model.predict(test_ds)\n",
    "predictions_simple_cnn_model=np.argmax(predicted_probabilities, axis=1)\n",
    "test_labels = np.concatenate([y for x, y in test_ds], axis=0) \n",
    "compute_accuracy(test_labels,predictions_simple_cnn_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92b184f-5738-4f66-95fa-cf432f3aeb13",
   "metadata": {},
   "source": [
    "The results are amazing, we increased the accuracy by 16%!<br>\n",
    "It seems that data augmentation yielded the greatest improvement by far.\n",
    "Due to the long training time on so much data, my lack of computing power and lack of time left, I am not able to persue more experiments, but I think the results are great. We achieved our goal of at least 70% accuracy, and actually greatly overcomed it!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
